# â”€â”€â”€ IMPORTS NUEVOS (sin deprecations) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain_chroma import Chroma  # nuevo vectorstore
from langchain_ollama import OllamaEmbeddings, OllamaLLM  # embeddings y LLM
from langchain.chains import RetrievalQA

CHROMA_PATH = "db"  # carpeta con la BD

# 1ï¸âƒ£ Embeddings correctos
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# 2ï¸âƒ£ Cargamos la base Chroma
vectordb = Chroma(
    persist_directory=CHROMA_PATH,
    embedding_function=embeddings,
)

# 3ï¸âƒ£ LLM local (Mistral)
llm = OllamaLLM(model="mistral")

# 4ï¸âƒ£ Cadena RAG: retrieval + generaciÃ³n
qa = RetrievalQA.from_chain_type(
    llm=llm, retriever=vectordb.as_retriever(), chain_type="stuff"
)

print("\nğŸ’¬ RAG activo â€” escribÃ­ 'salir' para terminar.\n")

while True:
    pregunta = input("ğŸ§‘ TÃº: ")
    if pregunta.lower() in {"salir", "exit", "quit"}:
        print("ğŸ‘‹ Â¡Hasta luego!")
        break

    respuesta = qa.invoke(pregunta)  # invoke es la nueva forma recomendada
    texto = respuesta["result"] if isinstance(respuesta, dict) else respuesta
    print(f"ğŸ¤– Mistral: {texto}\n")
