# MÃ³dulo 3 â€“ RAG con LangChain y Ollama

Este mÃ³dulo implementa un sistema de Retrieval-Augmented Generation (RAG) usando LangChain, Ollama y ChromaDB.

## ğŸ“ Estructura

- `data/`: Carpeta donde se colocan los documentos `.txt` que se van a indexar.
- `db/`: Carpeta generada automÃ¡ticamente con la base vectorial persistente (Chroma).
- `ingest.py`: Lee los documentos, genera embeddings y construye el vector store.
- `main.py`: Ejecuta una consola interactiva donde el usuario puede hacer preguntas usando RAG.

## â–¶ï¸ CÃ³mo usar

1. **Ingestar documentos**
   ```bash
   python ingest.py
   ```
   Esto genera los vectores en base a los archivos dentro de `data/`.

2. **Ejecutar chat RAG**
   ```bash
   python main.py
   ```
   Inicia una consola donde podÃ©s preguntar en lenguaje natural. EscribÃ­ `salir` para terminar.

## ğŸ§  TecnologÃ­as

- **LangChain** (v0.3.x)
- **Ollama** (`mistral` como LLM, `nomic-embed-text` como embedder)
- **ChromaDB** para almacenamiento vectorial local
- **Python 3.10+**

## âœ… Estado

âœ”ï¸ MÃ³dulo completo y funcional.  
ğŸ“Œ PrÃ³ximos pasos opcionales:
- Manejo de mÃºltiples usuarios
- Interfaz web o API
- Mejor control de contexto y tokens
